#!/usr/bin/env python3
"""
Continuous Training System - æŒçºŒè¨“ç·´ç³»çµ±
åŸºæ–¼äººå·¥æ¨™è¨»æ•¸æ“šæŒçºŒæ”¹é€² XGBoost æ¨¡å‹

è¨­è¨ˆåŸå‰‡:
- ç¬¬ä¸€æ€§åŸç†: äººé¡æ¨™è¨»æ˜¯çœŸç›¸ï¼Œæ©Ÿå™¨å­¸ç¿’é€¼è¿‘çœŸç›¸
- æ²™çš‡ç‚¸å½ˆç´”åº¦: åªä½¿ç”¨é«˜è³ªé‡æ¨™è¨»ï¼ˆä¿¡å¿ƒ >= 4ï¼‰
- çŒ›ç¦½3è¿­ä»£: å¿«é€Ÿæ¸¬è©¦ â†’ éƒ¨ç½² â†’ æŒçºŒæ”¹é€²

è¨“ç·´è§¸ç™¼æ¢ä»¶:
- ç´¯ç© >= 100 æ¢é«˜è³ªé‡æ¨™è¨»
- æ¯ 100 æ¢è§¸ç™¼ä¸€æ¬¡é‡è¨“ç·´

A/B æ¸¬è©¦:
- æ–°æ¨¡å‹ vs èˆŠæ¨¡å‹
- æ€§èƒ½æå‡ >= 2% â†’ éƒ¨ç½²æ–°æ¨¡å‹
"""

import os
import sys
import json
import logging
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import numpy as np

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.human_annotator import AnnotationDatabase

# å˜—è©¦å°å…¥ XGBoost
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    logging.warning("XGBoost æœªå®‰è£ï¼ŒæŒçºŒè¨“ç·´åŠŸèƒ½ä¸å¯ç”¨")

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class TrainingMetrics:
    """è¨“ç·´æ€§èƒ½æŒ‡æ¨™"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    auc_roc: float
    model_version: str
    training_samples: int
    timestamp: float


class ContinuousTrainer:
    """æŒçºŒè¨“ç·´ç®¡ç†å™¨"""

    def __init__(
        self,
        annotation_db_path: str = "data/annotations.db",
        model_save_dir: str = "models",
        min_confidence: int = 4,
        retrain_threshold: int = 100
    ):
        """
        Args:
            annotation_db_path: æ¨™è¨»æ•¸æ“šåº«è·¯å¾‘
            model_save_dir: æ¨¡å‹ä¿å­˜ç›®éŒ„
            min_confidence: æœ€ä½ä¿¡å¿ƒç­‰ç´šï¼ˆ1-5ï¼‰
            retrain_threshold: é‡è¨“ç·´é–¾å€¼ï¼ˆæ¨™è¨»æ•¸é‡ï¼‰
        """
        self.db = AnnotationDatabase(annotation_db_path)
        self.model_save_dir = Path(project_root) / model_save_dir
        self.model_save_dir.mkdir(parents=True, exist_ok=True)
        self.min_confidence = min_confidence
        self.retrain_threshold = retrain_threshold

        logger.info(f"æŒçºŒè¨“ç·´å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  â€¢ æœ€ä½ä¿¡å¿ƒç­‰ç´š: {min_confidence}")
        logger.info(f"  â€¢ é‡è¨“ç·´é–¾å€¼: {retrain_threshold} æ¢æ¨™è¨»")

    def check_and_retrain(self, force: bool = False) -> Optional[str]:
        """
        æª¢æŸ¥æ˜¯å¦é”åˆ°é‡è¨“ç·´æ¢ä»¶ï¼Œå¦‚æœé”åˆ°å‰‡åŸ·è¡Œè¨“ç·´

        Args:
            force: å¼·åˆ¶é‡è¨“ç·´ï¼ˆå¿½ç•¥é–¾å€¼æª¢æŸ¥ï¼‰

        Returns:
            æ–°æ¨¡å‹è·¯å¾‘ï¼Œå¦‚æœæœªè¨“ç·´å‰‡è¿”å› None
        """
        if not XGBOOST_AVAILABLE:
            logger.error("XGBoost æœªå®‰è£ï¼Œç„¡æ³•åŸ·è¡Œè¨“ç·´")
            return None

        # ç²å–çµ±è¨ˆä¿¡æ¯
        stats = self.db.get_annotation_stats()
        pending = stats['pending_training']
        high_quality = stats['high_quality']

        logger.info(f"æª¢æŸ¥è¨“ç·´æ¢ä»¶:")
        logger.info(f"  â€¢ é«˜è³ªé‡æ¨™è¨»: {high_quality}")
        logger.info(f"  â€¢ å¾…è¨“ç·´æ¨™è¨»: {pending}")
        logger.info(f"  â€¢ é‡è¨“ç·´é–¾å€¼: {self.retrain_threshold}")

        # æª¢æŸ¥æ˜¯å¦é”åˆ°é–¾å€¼
        if not force and pending < self.retrain_threshold:
            logger.info(f"æœªé”åˆ°é‡è¨“ç·´é–¾å€¼ ({pending} < {self.retrain_threshold})")
            return None

        logger.info("âœ… é”åˆ°é‡è¨“ç·´æ¢ä»¶ï¼Œé–‹å§‹è¨“ç·´æ–°æ¨¡å‹...")

        # åŠ è¼‰è¨“ç·´æ•¸æ“š
        X, y, annotation_ids = self.prepare_training_data()

        if len(X) == 0:
            logger.error("æ²’æœ‰å¯ç”¨çš„è¨“ç·´æ•¸æ“š")
            return None

        logger.info(f"è¨“ç·´æ•¸æ“šæº–å‚™å®Œæˆ: {len(X)} æ¨£æœ¬")

        # è¨“ç·´æ–°æ¨¡å‹
        new_model = self.train_xgboost(X, y)

        # ä¿å­˜æ–°æ¨¡å‹
        import time
        model_version = f"xgboost_v{int(time.time())}"
        model_path = self.model_save_dir / f"{model_version}.pkl"

        with open(model_path, 'wb') as f:
            pickle.dump(new_model, f)

        logger.info(f"æ–°æ¨¡å‹å·²ä¿å­˜: {model_path}")

        # A/B æ¸¬è©¦ï¼ˆå¦‚æœæœ‰èˆŠæ¨¡å‹ï¼‰
        current_model_path = self.model_save_dir / "xgboost_current.pkl"
        if current_model_path.exists():
            improvement = self.ab_test(new_model, current_model_path, X, y)

            if improvement >= 0.02:  # 2% æå‡
                logger.info(f"âœ… æ–°æ¨¡å‹æ€§èƒ½æå‡ {improvement*100:.1f}%ï¼Œéƒ¨ç½²æ–°æ¨¡å‹")
                self.deploy_model(model_path)
            else:
                logger.info(f"âš ï¸  æ–°æ¨¡å‹æ€§èƒ½æå‡ä¸è¶³ ({improvement*100:.1f}% < 2%)ï¼Œä¿ç•™èˆŠæ¨¡å‹")
        else:
            logger.info("é¦–æ¬¡è¨“ç·´ï¼Œç›´æ¥éƒ¨ç½²æ–°æ¨¡å‹")
            self.deploy_model(model_path)

        # æ¨™è¨˜æ¨™è¨»ç‚ºå·²ä½¿ç”¨
        self.db.mark_as_used_for_training(annotation_ids)

        return str(model_path)

    def prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray, List[int]]:
        """
        å¾æ•¸æ“šåº«åŠ è¼‰ä¸¦æº–å‚™è¨“ç·´æ•¸æ“š

        Returns:
            (X, y, annotation_ids)
            X: ç‰¹å¾µçŸ©é™£ (N, 25)
            y: æ¨™ç±¤å‘é‡ (N,) - 0=Real, 1=AI
            annotation_ids: æ¨™è¨»IDåˆ—è¡¨
        """
        # ç²å–é«˜è³ªé‡æ¨™è¨»
        annotations = self.db.get_high_quality_annotations(self.min_confidence)

        if len(annotations) == 0:
            logger.warning("æ²’æœ‰å¯ç”¨çš„é«˜è³ªé‡æ¨™è¨»")
            return np.array([]), np.array([]), []

        X_list = []
        y_list = []
        annotation_ids = []

        for ann in annotations:
            # è§£æ SHAP åŸå› ï¼ˆç‰¹å¾µåˆ†æ•¸ï¼‰
            try:
                shap_reasons = json.loads(ann['shap_top_reasons'])

                # æ§‹å»ºç‰¹å¾µå‘é‡ï¼ˆç°¡åŒ–ç‰ˆï¼šä½¿ç”¨ AI é æ¸¬ + Top 3 SHAP åˆ†æ•¸ï¼‰
                # å¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦æå–å®Œæ•´çš„ 12 æ¨¡çµ„åˆ†æ•¸ + å…ƒæ•¸æ“š
                features = [
                    ann['ai_prediction'] / 100.0,  # æ­¸ä¸€åŒ–åˆ° 0-1
                    ann['ai_confidence']
                ]

                # æ·»åŠ  Top 3 SHAP åˆ†æ•¸ï¼ˆå¦‚æœä¸è¶³3å€‹å‰‡å¡«0ï¼‰
                for i in range(3):
                    if i < len(shap_reasons):
                        features.append(shap_reasons[i][1] / 100.0)  # æ­¸ä¸€åŒ–
                    else:
                        features.append(0.0)

                # è£œé½Šåˆ° 25 ç¶­ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦å®Œæ•´ç‰¹å¾µï¼‰
                while len(features) < 25:
                    features.append(0.0)

                X_list.append(features[:25])

                # æ¨™ç±¤ï¼šreal=0, ai=1
                label = 1 if ann['human_label'] == 'ai' else 0
                y_list.append(label)

                annotation_ids.append(ann['id'])

            except Exception as e:
                logger.error(f"è§£ææ¨™è¨»å¤±æ•— (ID={ann['id']}): {e}")
                continue

        X = np.array(X_list)
        y = np.array(y_list)

        logger.info(f"è¨“ç·´æ•¸æ“šæº–å‚™å®Œæˆ:")
        logger.info(f"  â€¢ æ¨£æœ¬æ•¸: {len(X)}")
        logger.info(f"  â€¢ Real: {np.sum(y == 0)} å€‹")
        logger.info(f"  â€¢ AI: {np.sum(y == 1)} å€‹")

        return X, y, annotation_ids

    def train_xgboost(self, X: np.ndarray, y: np.ndarray) -> xgb.XGBClassifier:
        """
        è¨“ç·´ XGBoost åˆ†é¡å™¨

        Args:
            X: ç‰¹å¾µçŸ©é™£ (N, 25)
            y: æ¨™ç±¤å‘é‡ (N,)

        Returns:
            è¨“ç·´å¥½çš„ XGBoost æ¨¡å‹
        """
        logger.info("é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹...")

        # XGBoost åƒæ•¸ï¼ˆåŸºæ–¼æ²™çš‡ç‚¸å½ˆåŸå‰‡ï¼šé«˜ç´”åº¦æª¢æ¸¬ï¼‰
        params = {
            'max_depth': 6,  # æ·±åº¦é™åˆ¶ï¼Œé˜²æ­¢éæ“¬åˆ
            'learning_rate': 0.1,  # å­¸ç¿’ç‡
            'n_estimators': 100,  # æ¨¹çš„æ•¸é‡
            'objective': 'binary:logistic',  # äºŒåˆ†é¡
            'eval_metric': 'auc',  # AUCè©•ä¼°
            'random_state': 42,
            'tree_method': 'hist',  # GPUåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        }

        model = xgb.XGBClassifier(**params)

        # è¨“ç·´
        model.fit(
            X, y,
            eval_set=[(X, y)],
            verbose=False
        )

        logger.info("âœ… XGBoost è¨“ç·´å®Œæˆ")

        # é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§
        feature_importance = model.feature_importances_
        top_features = np.argsort(feature_importance)[::-1][:5]
        logger.info("Top 5 é‡è¦ç‰¹å¾µ:")
        for i, idx in enumerate(top_features, 1):
            logger.info(f"  {i}. Feature {idx}: {feature_importance[idx]:.3f}")

        return model

    def ab_test(
        self,
        new_model: xgb.XGBClassifier,
        old_model_path: Path,
        X_test: np.ndarray,
        y_test: np.ndarray
    ) -> float:
        """
        A/B æ¸¬è©¦ï¼šæ¯”è¼ƒæ–°èˆŠæ¨¡å‹æ€§èƒ½

        Args:
            new_model: æ–°è¨“ç·´çš„æ¨¡å‹
            old_model_path: èˆŠæ¨¡å‹è·¯å¾‘
            X_test: æ¸¬è©¦ç‰¹å¾µ
            y_test: æ¸¬è©¦æ¨™ç±¤

        Returns:
            æ€§èƒ½æå‡æ¯”ä¾‹ï¼ˆ0-1ï¼‰
        """
        logger.info("é–‹å§‹ A/B æ¸¬è©¦...")

        # åŠ è¼‰èˆŠæ¨¡å‹
        with open(old_model_path, 'rb') as f:
            old_model = pickle.load(f)

        # æ–°æ¨¡å‹é æ¸¬
        new_pred = new_model.predict(X_test)
        new_acc = np.mean(new_pred == y_test)

        # èˆŠæ¨¡å‹é æ¸¬
        old_pred = old_model.predict(X_test)
        old_acc = np.mean(old_pred == y_test)

        improvement = new_acc - old_acc

        logger.info(f"A/B æ¸¬è©¦çµæœ:")
        logger.info(f"  â€¢ èˆŠæ¨¡å‹æº–ç¢ºç‡: {old_acc*100:.2f}%")
        logger.info(f"  â€¢ æ–°æ¨¡å‹æº–ç¢ºç‡: {new_acc*100:.2f}%")
        logger.info(f"  â€¢ æ€§èƒ½æå‡: {improvement*100:.2f}%")

        return improvement

    def deploy_model(self, model_path: Path):
        """
        éƒ¨ç½²æ–°æ¨¡å‹ï¼ˆè¤‡è£½ç‚º current ç‰ˆæœ¬ï¼‰

        Args:
            model_path: æ–°æ¨¡å‹è·¯å¾‘
        """
        current_path = self.model_save_dir / "xgboost_current.pkl"

        # å‚™ä»½èˆŠæ¨¡å‹
        if current_path.exists():
            import time
            backup_path = self.model_save_dir / f"xgboost_backup_{int(time.time())}.pkl"
            current_path.rename(backup_path)
            logger.info(f"èˆŠæ¨¡å‹å·²å‚™ä»½: {backup_path}")

        # è¤‡è£½æ–°æ¨¡å‹
        import shutil
        shutil.copy(model_path, current_path)
        logger.info(f"âœ… æ–°æ¨¡å‹å·²éƒ¨ç½²: {current_path}")

    def get_training_history(self) -> List[Dict]:
        """ç²å–è¨“ç·´æ­·å²ï¼ˆå¾æ¨¡å‹ç›®éŒ„ï¼‰"""
        models = list(self.model_save_dir.glob("xgboost_v*.pkl"))
        history = []

        for model_path in sorted(models):
            version = model_path.stem
            timestamp = int(version.split('_v')[-1])
            history.append({
                'version': version,
                'path': str(model_path),
                'timestamp': timestamp,
                'size_mb': model_path.stat().st_size / (1024 * 1024)
            })

        return sorted(history, key=lambda x: x['timestamp'], reverse=True)

    def show_training_status(self):
        """é¡¯ç¤ºè¨“ç·´ç‹€æ…‹"""
        print(f"\n{'='*80}")
        print(f"{'æŒçºŒè¨“ç·´ç‹€æ…‹'.center(80)}")
        print(f"{'='*80}")

        # æ¨™è¨»çµ±è¨ˆ
        stats = self.db.get_annotation_stats()
        print(f"\nğŸ“Š æ¨™è¨»æ•¸æ“š:")
        print(f"  â€¢ é«˜è³ªé‡æ¨™è¨»: {stats['high_quality']}")
        print(f"  â€¢ å·²ç”¨æ–¼è¨“ç·´: {stats['used_for_training']}")
        print(f"  â€¢ å¾…è¨“ç·´: {stats['pending_training']}")
        print(f"  â€¢ é‡è¨“ç·´é–¾å€¼: {self.retrain_threshold}")

        progress = stats['pending_training'] / self.retrain_threshold
        bar_length = 50
        filled = int(bar_length * min(progress, 1.0))
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        print(f"  â€¢ é€²åº¦: [{bar}] {progress*100:.1f}%")

        # è¨“ç·´æ­·å²
        history = self.get_training_history()
        if history:
            print(f"\nğŸ“š è¨“ç·´æ­·å² (æœ€è¿‘5æ¬¡):")
            for i, record in enumerate(history[:5], 1):
                import datetime
                dt = datetime.datetime.fromtimestamp(record['timestamp'])
                print(f"  {i}. {record['version']}")
                print(f"     æ™‚é–“: {dt.strftime('%Y-%m-%d %H:%M:%S')}")
                print(f"     å¤§å°: {record['size_mb']:.2f} MB")
        else:
            print(f"\nğŸ“š è¨“ç·´æ­·å²: å°šæœªè¨“ç·´")

        # ç•¶å‰æ¨¡å‹
        current_path = self.model_save_dir / "xgboost_current.pkl"
        if current_path.exists():
            size_mb = current_path.stat().st_size / (1024 * 1024)
            print(f"\nğŸ¯ ç•¶å‰éƒ¨ç½²æ¨¡å‹:")
            print(f"  â€¢ è·¯å¾‘: {current_path}")
            print(f"  â€¢ å¤§å°: {size_mb:.2f} MB")
        else:
            print(f"\nğŸ¯ ç•¶å‰éƒ¨ç½²æ¨¡å‹: ç„¡")

        print(f"\n{'='*80}\n")


def main():
    """æ¸¬è©¦æŒçºŒè¨“ç·´ç³»çµ±"""
    print("TSAR-RAPTOR Continuous Training System - æŒçºŒè¨“ç·´ç³»çµ±æ¸¬è©¦")
    print("="*80)

    if not XGBOOST_AVAILABLE:
        print("âŒ XGBoost æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£: pip install xgboost")
        return

    # å‰µå»ºè¨“ç·´å™¨
    trainer = ContinuousTrainer(
        retrain_threshold=10  # æ¸¬è©¦ç”¨è¼ƒä½é–¾å€¼
    )

    # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
    trainer.show_training_status()

    # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡è¨“ç·´
    print("\næª¢æŸ¥é‡è¨“ç·´æ¢ä»¶...")
    new_model_path = trainer.check_and_retrain(force=False)

    if new_model_path:
        print(f"\nâœ… è¨“ç·´å®Œæˆï¼Œæ–°æ¨¡å‹: {new_model_path}")
        trainer.show_training_status()
    else:
        print(f"\nâ¸ï¸  æœªé”åˆ°é‡è¨“ç·´æ¢ä»¶")


if __name__ == "__main__":
    main()
