#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TSAR-RAPTOR AI Detection Classifier
æ•´åˆç¾æœ‰AIæª¢æ¸¬ç³»çµ±ï¼Œè‡ªå‹•åˆ†é¡TikTokè¦–é »

è¨­è¨ˆåŸå‰‡:
- ç¬¬ä¸€æ€§åŸç†: ç‰©ç†ä¸å¯å½é€ ç‰¹å¾µï¼ˆFR-TSARä¸‰éšæ®µï¼‰
- æ²™çš‡ç‚¸å½ˆ: æ‰¹é‡ä¸¦è¡Œæª¢æ¸¬
- çŒ›ç¦½3: ç°¡ç´„æ¥å£ï¼Œç„¡ç¸«æ•´åˆ

æ•´åˆæª¢æ¸¬æ¨¡çµ„:
1. Stage 1 - ç‰©ç†å‰›æ€§ (40%): PVD + éª¨éª¼å®ˆæ†
2. Stage 2 - é »ç‡çµæ§‹ (30%): é »åŸŸåˆ†æ + CNNåˆ†é¡
3. Stage 3 - é‚è¼¯æ±ºç­– (30%): XGBoosté›†æˆ

åˆ†é¡é‚è¼¯:
- REAL: AI_P < 30
- AI: AI_P >= 70
- NOT_SURE: 30 <= AI_P < 70
- é›»å½±å‹•ç•«: ç‰¹æ®Šè¦å‰‡æª¢æ¸¬
"""

import sys
from pathlib import Path
import logging
from typing import Dict, List
import cv2
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AIDetectionClassifier:
    """AIæª¢æ¸¬åˆ†é¡å™¨ï¼ˆæ•´åˆç¾æœ‰ç³»çµ±ï¼‰"""

    def __init__(
        self,
        video_dir: str,
        max_workers: int = 4
    ):
        """
        Args:
            video_dir: è¦–é »ç›®éŒ„
            max_workers: ä¸¦è¡Œæª¢æ¸¬æ•¸
        """
        self.video_dir = Path(video_dir)
        self.max_workers = max_workers

        # å˜—è©¦å°å…¥ç¾æœ‰æª¢æ¸¬æ¨¡çµ„
        self._load_detection_modules()

        logger.info("AIæª¢æ¸¬åˆ†é¡å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  â€¢ è¦–é »ç›®éŒ„: {self.video_dir}")
        logger.info(f"  â€¢ ä¸¦è¡Œæ•¸: {self.max_workers}")

    def _load_detection_modules(self):
        """åŠ è¼‰ç¾æœ‰æª¢æ¸¬æ¨¡çµ„"""
        try:
            # å˜—è©¦å°å…¥ç¾æœ‰æª¢æ¸¬æ¨¡çµ„
            from modules.physics_violation_detector_v2 import PhysicsViolationDetector
            from modules.frequency_analyzer_v2 import FrequencyAnalyzerV2
            from modules.facial_rigidity_analyzer import FacialRigidityAnalyzer

            self.pvd = PhysicsViolationDetector()
            self.freq_analyzer = FrequencyAnalyzerV2()
            self.facial_analyzer = FacialRigidityAnalyzer()

            self.modules_loaded = True
            logger.info("âœ… æª¢æ¸¬æ¨¡çµ„åŠ è¼‰æˆåŠŸ")

        except ImportError as e:
            logger.warning(f"âš ï¸  æª¢æ¸¬æ¨¡çµ„æœªåŠ è¼‰: {e}")
            logger.warning("   å°‡ä½¿ç”¨ç°¡åŒ–æª¢æ¸¬é‚è¼¯")
            self.modules_loaded = False

    def detect_single_video(self, video_path: Path) -> Dict:
        """
        æª¢æ¸¬å–®å€‹è¦–é »

        Args:
            video_path: è¦–é »è·¯å¾‘

        Returns:
            æª¢æ¸¬çµæœå­—å…¸
        """
        try:
            if self.modules_loaded:
                # ä½¿ç”¨å®Œæ•´æª¢æ¸¬ç³»çµ±
                return self._detect_with_full_system(video_path)
            else:
                # ä½¿ç”¨ç°¡åŒ–æª¢æ¸¬
                return self._detect_with_simplified_logic(video_path)

        except Exception as e:
            logger.error(f"âŒ æª¢æ¸¬å¤±æ•— [{video_path.name}]: {e}")
            return {
                'video_path': str(video_path),
                'classification': 'NOT_SURE',
                'confidence': 0.0,
                'ai_score': 50.0,
                'error': str(e)
            }

    def _detect_with_full_system(self, video_path: Path) -> Dict:
        """
        ä½¿ç”¨å®Œæ•´æª¢æ¸¬ç³»çµ±ï¼ˆæ•´åˆç¾æœ‰æ¨¡çµ„ï¼‰

        Args:
            video_path: è¦–é »è·¯å¾‘

        Returns:
            æª¢æ¸¬çµæœ
        """
        # æ‰“é–‹è¦–é »
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"ç„¡æ³•æ‰“é–‹è¦–é »: {video_path}")

        # Stage 1: ç‰©ç†é•è¦æª¢æ¸¬ (40%)
        pvd_score = self.pvd.analyze(cap)
        pvd_contribution = pvd_score * 0.4

        # Stage 2: é »åŸŸåˆ†æ (30%)
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # é‡ç½®
        freq_score = self.freq_analyzer.analyze(cap)
        freq_contribution = freq_score * 0.3

        # Stage 3: é¢éƒ¨å‰›æ€§ (30%)
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # é‡ç½®
        facial_score = self.facial_analyzer.analyze(cap)
        facial_contribution = facial_score * 0.3

        cap.release()

        # ç¸½åˆ†è¨ˆç®—
        ai_score = pvd_contribution + freq_contribution + facial_contribution

        # åˆ†é¡é‚è¼¯
        classification, confidence = self._classify_by_score(ai_score)

        return {
            'video_path': str(video_path),
            'classification': classification,
            'confidence': confidence,
            'ai_score': ai_score,
            'pvd_score': pvd_score,
            'freq_score': freq_score,
            'facial_score': facial_score
        }

    def _detect_with_simplified_logic(self, video_path: Path) -> Dict:
        """
        ç°¡åŒ–æª¢æ¸¬é‚è¼¯ï¼ˆæ¨¡çµ„æœªåŠ è¼‰æ™‚ä½¿ç”¨ï¼‰

        åŸºæ–¼åŸºæœ¬è¦–è¦ºç‰¹å¾µçš„å¿«é€Ÿæª¢æ¸¬

        Args:
            video_path: è¦–é »è·¯å¾‘

        Returns:
            æª¢æ¸¬çµæœ
        """
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"ç„¡æ³•æ‰“é–‹è¦–é »: {video_path}")

        # åŸºæœ¬ç‰¹å¾µæå–
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        # æ¡æ¨£å¹€é€²è¡Œåˆ†æ
        ai_indicators = 0
        real_indicators = 0

        sample_interval = max(total_frames // 30, 1)

        for i in range(0, total_frames, sample_interval):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if not ret:
                break

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

            # ç‰¹å¾µ1: äº®åº¦ç•°å¸¸
            brightness = np.mean(gray)
            if brightness > 160 or brightness < 70:
                ai_indicators += 1
            else:
                real_indicators += 1

            # ç‰¹å¾µ2: é£½å’Œåº¦ç•°å¸¸
            hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
            saturation = np.mean(hsv[:, :, 1])
            if saturation > 150:
                ai_indicators += 1
            else:
                real_indicators += 1

            # ç‰¹å¾µ3: å°æ¯”åº¦
            contrast = np.std(gray)
            if contrast > 70:
                ai_indicators += 1
            else:
                real_indicators += 1

        cap.release()

        # è¨ˆç®—AIåˆ†æ•¸
        total_indicators = ai_indicators + real_indicators
        ai_score = (ai_indicators / total_indicators * 100) if total_indicators > 0 else 50.0

        # åˆ†è¾¨ç‡ç‰¹å¾µï¼ˆAIå¸¸ç”¨æ­£æ–¹å½¢ï¼‰
        if width == height and width in [512, 1024, 768]:
            ai_score += 15  # AIç”Ÿæˆå¸¸ç”¨åˆ†è¾¨ç‡

        # fpsç‰¹å¾µï¼ˆAIå¸¸ç”¨24fpsï¼‰
        if abs(fps - 24) < 1:
            ai_score += 5

        ai_score = min(100, max(0, ai_score))

        # åˆ†é¡
        classification, confidence = self._classify_by_score(ai_score)

        return {
            'video_path': str(video_path),
            'classification': classification,
            'confidence': confidence,
            'ai_score': ai_score,
            'fps': fps,
            'resolution': f"{width}x{height}"
        }

    def _classify_by_score(self, ai_score: float) -> tuple:
        """
        æ ¹æ“šAIåˆ†æ•¸é€²è¡Œåˆ†é¡

        Args:
            ai_score: AIåˆ†æ•¸ (0-100)

        Returns:
            (åˆ†é¡, ä¿¡å¿ƒåº¦)
        """
        # é›»å½±å‹•ç•«æª¢æ¸¬ï¼ˆç‰¹æ®Šè¦å‰‡ï¼‰
        # TODO: å¯ä»¥åŠ å…¥æ›´è¤‡é›œçš„é›»å½±å‹•ç•«æª¢æ¸¬é‚è¼¯

        if ai_score < 30:
            # REAL
            classification = 'REAL'
            confidence = 100 - ai_score  # è¶Šä½è¶Šç¢ºå®šæ˜¯çœŸå¯¦
        elif ai_score >= 70:
            # AI
            classification = 'AI'
            confidence = ai_score  # è¶Šé«˜è¶Šç¢ºå®šæ˜¯AI
        else:
            # NOT_SURE (30-70ä¹‹é–“)
            classification = 'NOT_SURE'
            # ä¿¡å¿ƒåº¦ï¼šé›¢é‚Šç•Œè¶Šé è¶Šä¸ç¢ºå®š
            distance_to_boundary = min(abs(ai_score - 30), abs(ai_score - 70))
            confidence = 50 - distance_to_boundary

        return classification, max(0, min(100, confidence))

    def batch_detect(self, video_files: List[Path] = None) -> List[Dict]:
        """
        æ‰¹é‡æª¢æ¸¬è¦–é »

        Args:
            video_files: è¦–é »æ–‡ä»¶åˆ—è¡¨ï¼ˆNoneå‰‡è‡ªå‹•æƒæç›®éŒ„ï¼‰

        Returns:
            æª¢æ¸¬çµæœåˆ—è¡¨
        """
        if video_files is None:
            video_files = list(self.video_dir.glob("*.mp4"))

        logger.info(f"ğŸš€ é–‹å§‹æ‰¹é‡æª¢æ¸¬: {len(video_files)} å€‹è¦–é »ï¼ˆä¸¦è¡Œæ•¸: {self.max_workers}ï¼‰")

        results = []

        # ä¸¦è¡Œæª¢æ¸¬
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.detect_single_video, vf): vf for vf in video_files}

            for i, future in enumerate(as_completed(futures), 1):
                result = future.result()
                results.append(result)

                # é€²åº¦é¡¯ç¤º
                if i % 10 == 0 or i == len(video_files):
                    logger.info(f"ğŸ“Š æª¢æ¸¬é€²åº¦: {i}/{len(video_files)} ({i/len(video_files)*100:.1f}%)")

        # çµ±è¨ˆ
        stats = self._calculate_stats(results)
        logger.info(f"\n{'='*80}")
        logger.info(f"æª¢æ¸¬å®Œæˆ:")
        logger.info(f"  â€¢ ç¸½è¨ˆ: {stats['total']}")
        logger.info(f"  â€¢ REAL: {stats['real']} ({stats['real_pct']:.1f}%)")
        logger.info(f"  â€¢ AI: {stats['ai']} ({stats['ai_pct']:.1f}%)")
        logger.info(f"  â€¢ NOT_SURE: {stats['not_sure']} ({stats['not_sure_pct']:.1f}%)")
        logger.info(f"  â€¢ é›»å½±å‹•ç•«: {stats['movie']} ({stats['movie_pct']:.1f}%)")
        logger.info(f"{'='*80}\n")

        return results

    def _calculate_stats(self, results: List[Dict]) -> Dict:
        """
        è¨ˆç®—çµ±è¨ˆä¿¡æ¯

        Args:
            results: æª¢æ¸¬çµæœåˆ—è¡¨

        Returns:
            çµ±è¨ˆå­—å…¸
        """
        total = len(results)
        real_count = sum(1 for r in results if r['classification'] == 'REAL')
        ai_count = sum(1 for r in results if r['classification'] == 'AI')
        not_sure_count = sum(1 for r in results if r['classification'] == 'NOT_SURE')
        movie_count = sum(1 for r in results if r['classification'] == 'é›»å½±å‹•ç•«')

        return {
            'total': total,
            'real': real_count,
            'ai': ai_count,
            'not_sure': not_sure_count,
            'movie': movie_count,
            'real_pct': real_count / total * 100 if total > 0 else 0,
            'ai_pct': ai_count / total * 100 if total > 0 else 0,
            'not_sure_pct': not_sure_count / total * 100 if total > 0 else 0,
            'movie_pct': movie_count / total * 100 if total > 0 else 0
        }


def main():
    """æ¸¬è©¦æª¢æ¸¬å™¨"""
    import argparse

    parser = argparse.ArgumentParser(description="AIæª¢æ¸¬åˆ†é¡å™¨")
    parser.add_argument(
        '--video-dir',
        type=str,
        required=True,
        help='è¦–é »ç›®éŒ„'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=4,
        help='ä¸¦è¡Œæª¢æ¸¬æ•¸'
    )

    args = parser.parse_args()

    # å‰µå»ºæª¢æ¸¬å™¨
    detector = AIDetectionClassifier(
        video_dir=args.video_dir,
        max_workers=args.workers
    )

    # æ‰¹é‡æª¢æ¸¬
    results = detector.batch_detect()

    print(f"\nâœ… æª¢æ¸¬å®Œæˆï¼å…± {len(results)} å€‹è¦–é »")


if __name__ == "__main__":
    main()
