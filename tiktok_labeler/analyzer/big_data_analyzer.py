#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TSAR-RAPTOR Big Data Analyzer - Excel Cç”Ÿæˆå™¨
åˆ†æAI vs çœŸå¯¦è¦–é »çš„çµ±è¨ˆå·®ç•°

è¨­è¨ˆåŸå‰‡:
- ç¬¬ä¸€æ€§åŸç†: å¾æ•¸æ“šä¸­ç™¼ç¾ç‰©ç†è¦å¾‹
- æ²™çš‡ç‚¸å½ˆ: å¤šç¶­åº¦å°æ¯”ï¼Œç´šè¯åˆ†æ
- çŒ›ç¦½3: è‡ªå‹•åŒ–æ´å¯Ÿï¼Œå¯è¦–åŒ–è¼¸å‡º

Excel C åˆ†æå…§å®¹:
1. æè¿°æ€§çµ±è¨ˆ: å¹³å‡å€¼, æ¨™æº–å·®, æœ€å¤§å€¼, æœ€å°å€¼
2. å°æ¯”åˆ†æ: Real vs AI å„ç‰¹å¾µå·®ç•°
3. é¡¯è‘—æ€§æª¢é©—: t-test, æ•ˆæ‡‰é‡
4. ç‰¹å¾µæ’åº: æŒ‰å€åˆ†èƒ½åŠ›æ’åº
5. å¯è¦–åŒ–å»ºè­°: å“ªäº›ç‰¹å¾µæœ€èƒ½å€åˆ†AI
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List
from scipy import stats

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BigDataAnalyzer:
    """å¤§æ•¸æ“šåˆ†æå¼•æ“"""

    def __init__(
        self,
        excel_b_path: str,
        output_excel_c: str = "excel_c_analysis.xlsx"
    ):
        """
        Args:
            excel_b_path: Excel B è·¯å¾‘ï¼ˆç‰¹å¾µæ•¸æ“šï¼‰
            output_excel_c: Excel C è¼¸å‡ºè·¯å¾‘ï¼ˆåˆ†æçµæœï¼‰
        """
        self.excel_b_path = Path(excel_b_path)
        self.output_excel_c = Path(output_excel_c)

        logger.info("å¤§æ•¸æ“šåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  â€¢ Excel B: {self.excel_b_path}")
        logger.info(f"  â€¢ Excel C: {self.output_excel_c}")

    def load_data(self) -> pd.DataFrame:
        """åŠ è¼‰Excel Bæ•¸æ“š"""
        if not self.excel_b_path.exists():
            logger.error(f"âŒ Excel B ä¸å­˜åœ¨: {self.excel_b_path}")
            return pd.DataFrame()

        df = pd.read_excel(self.excel_b_path)
        logger.info(f"âœ… å·²åŠ è¼‰ {len(df)} æ¢æ•¸æ“š")

        # éæ¿¾ Real å’Œ AIï¼ˆæ’é™¤ Uncertainï¼‰
        df_filtered = df[df['label'].isin(['real', 'ai'])]
        logger.info(f"   éæ¿¾å¾Œ: Real={len(df_filtered[df_filtered['label']=='real'])}, "
                   f"AI={len(df_filtered[df_filtered['label']=='ai'])}")

        return df_filtered

    def descriptive_statistics(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æè¿°æ€§çµ±è¨ˆï¼ˆåˆ†çµ„ï¼šReal vs AIï¼‰

        Returns:
            DataFrame with columns: feature, real_mean, real_std, ai_mean, ai_std, difference
        """
        # æ•¸å€¼å‹ç‰¹å¾µåˆ—
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        # æ’é™¤ video_id ç­‰éç‰¹å¾µåˆ—
        exclude_cols = ['video_id', 'file_size_mb']
        feature_cols = [f for f in numeric_features if f not in exclude_cols]

        results = []

        for feature in feature_cols:
            real_data = df[df['label'] == 'real'][feature].dropna()
            ai_data = df[df['label'] == 'ai'][feature].dropna()

            if len(real_data) == 0 or len(ai_data) == 0:
                continue

            result = {
                'feature': feature,
                'real_mean': real_data.mean(),
                'real_std': real_data.std(),
                'real_min': real_data.min(),
                'real_max': real_data.max(),
                'ai_mean': ai_data.mean(),
                'ai_std': ai_data.std(),
                'ai_min': ai_data.min(),
                'ai_max': ai_data.max(),
                'difference': abs(real_data.mean() - ai_data.mean()),
                'difference_pct': abs(real_data.mean() - ai_data.mean()) / (real_data.mean() + 1e-10) * 100
            }

            results.append(result)

        df_stats = pd.DataFrame(results)
        return df_stats

    def significance_testing(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é¡¯è‘—æ€§æª¢é©—ï¼ˆt-test + æ•ˆæ‡‰é‡ï¼‰

        Returns:
            DataFrame with columns: feature, t_statistic, p_value, cohen_d, significant
        """
        numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
        exclude_cols = ['video_id', 'file_size_mb']
        feature_cols = [f for f in numeric_features if f not in exclude_cols]

        results = []

        for feature in feature_cols:
            real_data = df[df['label'] == 'real'][feature].dropna()
            ai_data = df[df['label'] == 'ai'][feature].dropna()

            if len(real_data) < 2 or len(ai_data) < 2:
                continue

            # t-test
            t_stat, p_value = stats.ttest_ind(real_data, ai_data)

            # Cohen's d (æ•ˆæ‡‰é‡)
            pooled_std = np.sqrt(((len(real_data)-1)*real_data.std()**2 + (len(ai_data)-1)*ai_data.std()**2) /
                                 (len(real_data) + len(ai_data) - 2))
            cohen_d = abs(real_data.mean() - ai_data.mean()) / (pooled_std + 1e-10)

            # é¡¯è‘—æ€§åˆ¤å®š
            significant = 'Yes' if p_value < 0.05 else 'No'

            # æ•ˆæ‡‰é‡å¤§å°
            if cohen_d >= 0.8:
                effect_size = 'Large'
            elif cohen_d >= 0.5:
                effect_size = 'Medium'
            elif cohen_d >= 0.2:
                effect_size = 'Small'
            else:
                effect_size = 'Negligible'

            result = {
                'feature': feature,
                't_statistic': t_stat,
                'p_value': p_value,
                'cohen_d': cohen_d,
                'effect_size': effect_size,
                'significant': significant
            }

            results.append(result)

        df_sig = pd.DataFrame(results)
        return df_sig

    def feature_ranking(self, df_stats: pd.DataFrame, df_sig: pd.DataFrame) -> pd.DataFrame:
        """
        ç‰¹å¾µæ’åºï¼ˆæŒ‰å€åˆ†èƒ½åŠ›ï¼‰

        æ’åºä¾æ“š:
        1. Cohen's d (æ•ˆæ‡‰é‡)
        2. p-value (é¡¯è‘—æ€§)
        3. Difference (çµ•å°å·®ç•°)

        Returns:
            DataFrame with top features
        """
        # åˆä½µçµ±è¨ˆå’Œé¡¯è‘—æ€§
        df_merged = pd.merge(df_stats, df_sig, on='feature', how='inner')

        # è¨ˆç®—ç¶œåˆåˆ†æ•¸
        # æ­¸ä¸€åŒ– Cohen's d, 1/p_value, difference_pct
        df_merged['cohen_d_norm'] = (df_merged['cohen_d'] - df_merged['cohen_d'].min()) / \
                                    (df_merged['cohen_d'].max() - df_merged['cohen_d'].min() + 1e-10)

        df_merged['p_value_inv'] = 1 / (df_merged['p_value'] + 1e-10)
        df_merged['p_value_inv_norm'] = (df_merged['p_value_inv'] - df_merged['p_value_inv'].min()) / \
                                        (df_merged['p_value_inv'].max() - df_merged['p_value_inv'].min() + 1e-10)

        df_merged['diff_pct_norm'] = (df_merged['difference_pct'] - df_merged['difference_pct'].min()) / \
                                     (df_merged['difference_pct'].max() - df_merged['difference_pct'].min() + 1e-10)

        # ç¶œåˆåˆ†æ•¸ (æ¬Šé‡: Cohen's d 50%, p-value 30%, difference 20%)
        df_merged['discrimination_score'] = (
            0.5 * df_merged['cohen_d_norm'] +
            0.3 * df_merged['p_value_inv_norm'] +
            0.2 * df_merged['diff_pct_norm']
        )

        # æ’åº
        df_ranked = df_merged.sort_values('discrimination_score', ascending=False)

        # æ·»åŠ æ’å
        df_ranked['rank'] = range(1, len(df_ranked) + 1)

        return df_ranked

    def generate_insights(self, df_ranked: pd.DataFrame) -> List[str]:
        """
        ç”Ÿæˆåˆ†ææ´å¯Ÿ

        Returns:
            æ´å¯Ÿåˆ—è¡¨
        """
        insights = []

        # Top 5 å€åˆ†ç‰¹å¾µ
        top_5 = df_ranked.head(5)
        insights.append("=== Top 5 å€åˆ†ç‰¹å¾µ ===")
        for _, row in top_5.iterrows():
            insights.append(
                f"{int(row['rank'])}. {row['feature']}: "
                f"Cohen's d={row['cohen_d']:.3f}, "
                f"p-value={row['p_value']:.4f}, "
                f"Real={row['real_mean']:.2f}, AI={row['ai_mean']:.2f}"
            )

        insights.append("")

        # é¡¯è‘—ç‰¹å¾µæ•¸é‡
        significant_count = len(df_ranked[df_ranked['significant'] == 'Yes'])
        insights.append(f"é¡¯è‘—ç‰¹å¾µæ•¸é‡: {significant_count} / {len(df_ranked)}")

        # Large effect size
        large_effect = df_ranked[df_ranked['effect_size'] == 'Large']
        insights.append(f"å¤§æ•ˆæ‡‰é‡ç‰¹å¾µ: {len(large_effect)}")

        insights.append("")

        # å»ºè­°
        insights.append("=== å„ªåŒ–å»ºè­° ===")
        if len(top_5) > 0:
            top_feature = top_5.iloc[0]
            insights.append(f"1. å¼·åŒ–æª¢æ¸¬æ¨¡çµ„: {top_feature['feature']} (æœ€å¼·å€åˆ†èƒ½åŠ›)")
            insights.append(f"2. èª¿æ•´é–¾å€¼: Realå¹³å‡={top_feature['real_mean']:.2f}, AIå¹³å‡={top_feature['ai_mean']:.2f}")

        if significant_count < len(df_ranked) * 0.5:
            insights.append("3. âš ï¸  é¡¯è‘—ç‰¹å¾µä¸è¶³50%ï¼Œå»ºè­°å¢åŠ æ›´å¤šæª¢æ¸¬ç¶­åº¦")

        return insights

    def analyze(self) -> Dict:
        """
        å®Œæ•´åˆ†ææµç¨‹

        Returns:
            åˆ†æçµæœå­—å…¸
        """
        # 1. åŠ è¼‰æ•¸æ“š
        df = self.load_data()
        if df.empty:
            logger.error("âŒ ç„¡å¯ç”¨æ•¸æ“š")
            return {}

        # 2. æè¿°æ€§çµ±è¨ˆ
        logger.info("ğŸ“Š è¨ˆç®—æè¿°æ€§çµ±è¨ˆ...")
        df_stats = self.descriptive_statistics(df)

        # 3. é¡¯è‘—æ€§æª¢é©—
        logger.info("ğŸ“Š é€²è¡Œé¡¯è‘—æ€§æª¢é©—...")
        df_sig = self.significance_testing(df)

        # 4. ç‰¹å¾µæ’åº
        logger.info("ğŸ“Š è¨ˆç®—ç‰¹å¾µæ’åº...")
        df_ranked = self.feature_ranking(df_stats, df_sig)

        # 5. ç”Ÿæˆæ´å¯Ÿ
        insights = self.generate_insights(df_ranked)

        # 6. ä¿å­˜åˆ°Excel Cï¼ˆå¤šå€‹Sheetï¼‰
        with pd.ExcelWriter(self.output_excel_c, engine='openpyxl') as writer:
            df_ranked.to_excel(writer, sheet_name='Feature_Ranking', index=False)
            df_stats.to_excel(writer, sheet_name='Descriptive_Stats', index=False)
            df_sig.to_excel(writer, sheet_name='Significance_Testing', index=False)

            # æ´å¯Ÿï¼ˆæ–‡æœ¬ï¼‰
            insights_df = pd.DataFrame({'Insights': insights})
            insights_df.to_excel(writer, sheet_name='Insights', index=False)

        logger.info(f"\nâœ… Excel C å·²ç”Ÿæˆ: {self.output_excel_c}")
        logger.info(f"   Sheet: Feature_Ranking, Descriptive_Stats, Significance_Testing, Insights")

        # æ‰“å°æ´å¯Ÿ
        print(f"\n{'='*80}")
        print("åˆ†ææ´å¯Ÿ:")
        print(f"{'='*80}")
        for insight in insights:
            print(insight)
        print(f"{'='*80}\n")

        return {
            'ranked_features': df_ranked,
            'stats': df_stats,
            'significance': df_sig,
            'insights': insights
        }


def main():
    """æ¸¬è©¦å¤§æ•¸æ“šåˆ†æå™¨"""
    import argparse

    parser = argparse.ArgumentParser(description="TikTokè¦–é »å¤§æ•¸æ“šåˆ†æå™¨")
    parser.add_argument(
        '--excel-b',
        type=str,
        default='../../data/tiktok_labels/excel_b_features.xlsx',
        help='Excel B è·¯å¾‘'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='../../data/tiktok_labels/excel_c_analysis.xlsx',
        help='è¼¸å‡º Excel C è·¯å¾‘'
    )

    args = parser.parse_args()

    # å‰µå»ºåˆ†æå™¨
    analyzer = BigDataAnalyzer(
        excel_b_path=args.excel_b,
        output_excel_c=args.output
    )

    # åŸ·è¡Œåˆ†æ
    results = analyzer.analyze()

    if results:
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"   Top 5 å€åˆ†ç‰¹å¾µ:")
        top_5 = results['ranked_features'].head(5)
        for i, (_, row) in enumerate(top_5.iterrows(), 1):
            print(f"      {i}. {row['feature']}: åˆ†æ•¸={row['discrimination_score']:.3f}")


if __name__ == "__main__":
    main()
