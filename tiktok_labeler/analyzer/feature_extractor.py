#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TSAR-RAPTOR Feature Extractor - Excel Bç”Ÿæˆå™¨
å°ä¸‹è¼‰çš„è¦–é »é€²è¡Œç‰¹å¾µæå–åˆ†æ

è¨­è¨ˆåŸå‰‡:
- ç¬¬ä¸€æ€§åŸç†: æå–ç‰©ç†å¯æ¸¬é‡ç‰¹å¾µ
- æ²™çš‡ç‚¸å½ˆ: å¤šç¶­åº¦ç‰¹å¾µï¼Œç´šè¯åˆ†æ
- çŒ›ç¦½3: é«˜æ•ˆä¸¦è¡Œï¼Œè¼•é‡ç´šæå–

Excel B ç‰¹å¾µåˆ—è¡¨:
1. åŸºæœ¬ä¿¡æ¯: video_id, label, filepath, file_size
2. è¦–é »ç‰¹å¾µ: å¹€ç‡, åˆ†è¾¨ç‡, æ™‚é•·, ç¢¼ç‡, ç¸½å¹€æ•¸
3. éŸ³é »ç‰¹å¾µ: æ¡æ¨£ç‡, è²é“æ•¸, éŸ³é »ç¢¼ç‡
4. è¦–è¦ºç‰¹å¾µ: å¹³å‡äº®åº¦, å°æ¯”åº¦, è‰²å½©é£½å’Œåº¦, æ¨¡ç³Šåº¦
5. é‹å‹•ç‰¹å¾µ: å…‰æµå¹³å‡å€¼, å ´æ™¯è®ŠåŒ–æ¬¡æ•¸
6. é »åŸŸç‰¹å¾µ: DCTèƒ½é‡, é »è­œç†µ
7. åƒè€ƒæ¨¡çµ„: èª¿ç”¨12å€‹æª¢æ¸¬æ¨¡çµ„çš„å¿«é€Ÿç‰ˆæœ¬
"""

import cv2
import numpy as np
import pandas as pd
from pathlib import Path
import logging
from typing import Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import subprocess
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FeatureExtractor:
    """è¦–é »ç‰¹å¾µæå–å™¨"""

    def __init__(
        self,
        video_dir: str,
        output_excel_b: str = "excel_b_features.xlsx",
        max_workers: int = 4,
        sample_frames: int = 30  # æ¡æ¨£å¹€æ•¸ï¼ˆè¼•é‡ç´šï¼‰
    ):
        """
        Args:
            video_dir: è¦–é »ç›®éŒ„
            output_excel_b: Excel B è¼¸å‡ºè·¯å¾‘
            max_workers: ä¸¦è¡Œè™•ç†æ•¸
            sample_frames: æ¡æ¨£å¹€æ•¸
        """
        self.video_dir = Path(video_dir)
        self.output_excel_b = Path(output_excel_b)
        self.max_workers = max_workers
        self.sample_frames = sample_frames

        logger.info("ç‰¹å¾µæå–å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  â€¢ è¦–é »ç›®éŒ„: {self.video_dir}")
        logger.info(f"  â€¢ è¼¸å‡º Excel B: {self.output_excel_b}")
        logger.info(f"  â€¢ æ¡æ¨£å¹€æ•¸: {self.sample_frames}")

    def extract_metadata(self, video_path: Path) -> Dict:
        """
        ä½¿ç”¨ ffprobe æå–å…ƒæ•¸æ“š

        Returns:
            {
                'duration': float,
                'fps': float,
                'width': int,
                'height': int,
                'bitrate': int,
                'audio_sample_rate': int,
                'audio_channels': int,
                ...
            }
        """
        try:
            cmd = [
                'ffprobe',
                '-v', 'quiet',
                '-print_format', 'json',
                '-show_format',
                '-show_streams',
                str(video_path)
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            data = json.loads(result.stdout)

            # æå–è¦–é »æµä¿¡æ¯
            video_stream = next((s for s in data['streams'] if s['codec_type'] == 'video'), {})
            audio_stream = next((s for s in data['streams'] if s['codec_type'] == 'audio'), {})

            # è§£æå¹€ç‡
            fps_str = video_stream.get('r_frame_rate', '0/1')
            fps_parts = fps_str.split('/')
            fps = float(fps_parts[0]) / float(fps_parts[1]) if len(fps_parts) == 2 and fps_parts[1] != '0' else 0

            return {
                'duration': float(data.get('format', {}).get('duration', 0)),
                'fps': fps,
                'width': int(video_stream.get('width', 0)),
                'height': int(video_stream.get('height', 0)),
                'bitrate': int(data.get('format', {}).get('bit_rate', 0)),
                'total_frames': int(video_stream.get('nb_frames', 0)),
                'audio_sample_rate': int(audio_stream.get('sample_rate', 0)),
                'audio_channels': int(audio_stream.get('channels', 0)),
                'audio_bitrate': int(audio_stream.get('bit_rate', 0)),
                'codec': video_stream.get('codec_name', 'unknown')
            }
        except Exception as e:
            logger.error(f"âŒ æå–å…ƒæ•¸æ“šå¤±æ•—: {video_path.name} | {e}")
            return {}

    def extract_visual_features(self, video_path: Path) -> Dict:
        """
        æå–è¦–è¦ºç‰¹å¾µï¼ˆæ¡æ¨£æ–¹å¼ï¼‰

        Returns:
            {
                'avg_brightness': float,
                'avg_contrast': float,
                'avg_saturation': float,
                'avg_blur': float,
                'avg_optical_flow': float,
                'scene_changes': int
            }
        """
        try:
            cap = cv2.VideoCapture(str(video_path))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            # è¨ˆç®—æ¡æ¨£é–“éš”
            if total_frames == 0:
                return {}

            step = max(total_frames // self.sample_frames, 1)

            brightness_list = []
            contrast_list = []
            saturation_list = []
            blur_list = []
            optical_flow_list = []
            prev_gray = None
            scene_changes = 0

            for i in range(0, total_frames, step):
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, frame = cap.read()
                if not ret:
                    break

                # è½‰æ›ç‚ºç°åº¦å’ŒHSV
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

                # äº®åº¦ï¼ˆVé€šé“å¹³å‡å€¼ï¼‰
                brightness = np.mean(hsv[:, :, 2])
                brightness_list.append(brightness)

                # å°æ¯”åº¦ï¼ˆæ¨™æº–å·®ï¼‰
                contrast = np.std(gray)
                contrast_list.append(contrast)

                # é£½å’Œåº¦ï¼ˆSé€šé“å¹³å‡å€¼ï¼‰
                saturation = np.mean(hsv[:, :, 1])
                saturation_list.append(saturation)

                # æ¨¡ç³Šåº¦ï¼ˆLaplacianæ–¹å·®ï¼Œè¶Šå°è¶Šæ¨¡ç³Šï¼‰
                laplacian = cv2.Laplacian(gray, cv2.CV_64F)
                blur = laplacian.var()
                blur_list.append(blur)

                # å…‰æµï¼ˆé‹å‹•ï¼‰
                if prev_gray is not None:
                    flow = cv2.calcOpticalFlowFarneback(
                        prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0
                    )
                    mag, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])
                    avg_flow = np.mean(mag)
                    optical_flow_list.append(avg_flow)

                    # å ´æ™¯è®ŠåŒ–ï¼ˆå¹€å·®ç•°å¤§æ–¼é–¾å€¼ï¼‰
                    frame_diff = np.mean(np.abs(gray.astype(float) - prev_gray.astype(float)))
                    if frame_diff > 30:  # é–¾å€¼
                        scene_changes += 1

                prev_gray = gray

            cap.release()

            return {
                'avg_brightness': np.mean(brightness_list) if brightness_list else 0,
                'avg_contrast': np.mean(contrast_list) if contrast_list else 0,
                'avg_saturation': np.mean(saturation_list) if saturation_list else 0,
                'avg_blur': np.mean(blur_list) if blur_list else 0,
                'avg_optical_flow': np.mean(optical_flow_list) if optical_flow_list else 0,
                'scene_changes': scene_changes
            }
        except Exception as e:
            logger.error(f"âŒ æå–è¦–è¦ºç‰¹å¾µå¤±æ•—: {video_path.name} | {e}")
            return {}

    def extract_frequency_features(self, video_path: Path) -> Dict:
        """
        æå–é »åŸŸç‰¹å¾µï¼ˆDCTï¼‰

        Returns:
            {
                'dct_energy': float,
                'spectral_entropy': float
            }
        """
        try:
            cap = cv2.VideoCapture(str(video_path))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            step = max(total_frames // 10, 1)  # æ¡æ¨£10å¹€

            dct_energies = []
            spectral_entropies = []

            for i in range(0, total_frames, step):
                cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                ret, frame = cap.read()
                if not ret:
                    break

                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

                # DCTè®Šæ›
                dct = cv2.dct(np.float32(gray) / 255.0)

                # DCTèƒ½é‡ï¼ˆé«˜é »åˆ†é‡ï¼‰
                dct_high_freq = dct[dct.shape[0]//2:, dct.shape[1]//2:]
                dct_energy = np.sum(dct_high_freq ** 2)
                dct_energies.append(dct_energy)

                # é »è­œç†µ
                dct_abs = np.abs(dct.flatten())
                dct_abs = dct_abs / (np.sum(dct_abs) + 1e-10)  # æ­¸ä¸€åŒ–
                spectral_entropy = -np.sum(dct_abs * np.log2(dct_abs + 1e-10))
                spectral_entropies.append(spectral_entropy)

            cap.release()

            return {
                'dct_energy': np.mean(dct_energies) if dct_energies else 0,
                'spectral_entropy': np.mean(spectral_entropies) if spectral_entropies else 0
            }
        except Exception as e:
            logger.error(f"âŒ æå–é »åŸŸç‰¹å¾µå¤±æ•—: {video_path.name} | {e}")
            return {}

    def extract_single_video(self, video_path: Path) -> Dict:
        """
        æå–å–®å€‹è¦–é »çš„å®Œæ•´ç‰¹å¾µ

        Returns:
            ç‰¹å¾µå­—å…¸
        """
        logger.info(f"ğŸ”¬ åˆ†æä¸­: {video_path.name}")

        # å¾æ–‡ä»¶åæå– label å’Œ video_id
        stem = video_path.stem  # e.g., "real_123" or "ai_456"
        parts = stem.split('_')
        label = parts[0] if len(parts) >= 2 else 'unknown'
        video_id = parts[1] if len(parts) >= 2 else 'unknown'

        # åŸºæœ¬ä¿¡æ¯
        features = {
            'video_id': video_id,
            'label': label,
            'filepath': str(video_path),
            'filename': video_path.name,
            'file_size_mb': video_path.stat().st_size / (1024 * 1024)
        }

        # å…ƒæ•¸æ“š
        metadata = self.extract_metadata(video_path)
        features.update(metadata)

        # è¦–è¦ºç‰¹å¾µ
        visual = self.extract_visual_features(video_path)
        features.update(visual)

        # é »åŸŸç‰¹å¾µ
        frequency = self.extract_frequency_features(video_path)
        features.update(frequency)

        logger.info(f"âœ… åˆ†æå®Œæˆ: {video_path.name}")
        return features

    def batch_extract(self) -> pd.DataFrame:
        """
        æ‰¹é‡æå–æ‰€æœ‰è¦–é »ç‰¹å¾µ

        Returns:
            DataFrame
        """
        video_files = list(self.video_dir.glob("*.mp4"))
        logger.info(f"ğŸš€ é–‹å§‹æ‰¹é‡æå–: {len(video_files)} å€‹è¦–é »")

        features_list = []

        # ä¸¦è¡Œæå–
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(self.extract_single_video, vf): vf for vf in video_files}

            for future in as_completed(futures):
                try:
                    features = future.result()
                    features_list.append(features)
                except Exception as e:
                    video_file = futures[future]
                    logger.error(f"âŒ æå–å¤±æ•—: {video_file.name} | {e}")

        # è½‰æ›ç‚ºDataFrame
        df = pd.DataFrame(features_list)

        # ä¿å­˜åˆ°Excel B
        df.to_excel(self.output_excel_b, index=False)
        logger.info(f"\nâœ… Excel B å·²ç”Ÿæˆ: {self.output_excel_b}")
        logger.info(f"   ç¸½è¨ˆ: {len(df)} å€‹è¦–é »")

        return df


def main():
    """æ¸¬è©¦ç‰¹å¾µæå–å™¨"""
    import argparse

    parser = argparse.ArgumentParser(description="TikTokè¦–é »ç‰¹å¾µæå–å™¨")
    parser.add_argument(
        '--input',
        type=str,
        default='../../data/tiktok_videos',
        help='è¦–é »ç›®éŒ„'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='../../data/tiktok_labels/excel_b_features.xlsx',
        help='è¼¸å‡º Excel B è·¯å¾‘'
    )
    parser.add_argument(
        '--workers',
        type=int,
        default=4,
        help='ä¸¦è¡Œè™•ç†æ•¸'
    )
    parser.add_argument(
        '--sample-frames',
        type=int,
        default=30,
        help='æ¡æ¨£å¹€æ•¸'
    )

    args = parser.parse_args()

    # å‰µå»ºæå–å™¨
    extractor = FeatureExtractor(
        video_dir=args.input,
        output_excel_b=args.output,
        max_workers=args.workers,
        sample_frames=args.sample_frames
    )

    # åŸ·è¡Œæå–
    df = extractor.batch_extract()

    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"\n{'='*80}")
    print(f"ç‰¹å¾µæå–å®Œæˆï¼")
    print(f"  â€¢ è¦–é »ç¸½æ•¸: {len(df)}")
    print(f"  â€¢ Real: {len(df[df['label'] == 'real'])}")
    print(f"  â€¢ AI: {len(df[df['label'] == 'ai'])}")
    print(f"  â€¢ Uncertain: {len(df[df['label'] == 'uncertain'])}")
    print(f"  â€¢ Excel B: {args.output}")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
