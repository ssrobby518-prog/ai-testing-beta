#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TSAR-RAPTOR Layer 1 Pipeline
äººå·¥ä¸»å°æ¨™è¨»å®Œæ•´æµæ°´ç·š

è¨­è¨ˆåŸå‰‡:
- ç¬¬ä¸€æ€§åŸç†: äººé¡åˆ¤å®š â†’ æ•¸æ“šåˆ†æ â†’ æ¨¡çµ„å„ªåŒ–
- æ²™çš‡ç‚¸å½ˆ: ç´šè¯å­¸ç¿’ï¼Œæ•¸æ“šé©…å‹•
- çŒ›ç¦½3: ä¸€éµåŸ·è¡Œï¼Œå…¨è‡ªå‹•

å®Œæ•´æµç¨‹:
1. Chromeæ“´å±•æ¨™è¨» â†’ Excel A
2. æ‰¹é‡ä¸‹è¼‰ä¸¦è‡ªå‹•åˆ†é¡åˆ°æ–‡ä»¶å¤¾
3. ç‰¹å¾µæå– â†’ Excel B
4. å¤§æ•¸æ“šåˆ†æ â†’ Excel C
5. æ¨¡çµ„è‡ªå‹•å„ªåŒ–
"""

import sys
from pathlib import Path
import logging
import argparse
from typing import Dict

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å°å…¥é…ç½®
from config import (
    EXCEL_A_PATH, EXCEL_B_PATH, EXCEL_C_PATH,
    LAYER1_BASE_DIR, LAYER1_DATA_DIR,
    ensure_directories
)

# å°å…¥å„çµ„ä»¶
from downloader.tiktok_downloader_classified import TikTokDownloaderClassified
from analyzer.feature_extractor_layer1 import FeatureExtractorLayer1
from analyzer.big_data_analyzer import BigDataAnalyzer
from auto_reconstructor.module_optimizer import ModuleOptimizer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Layer1Pipeline:
    """Layer 1 è‡ªæˆ‘å­¸ç¿’æµæ°´ç·šç¸½æ§"""

    def __init__(self):
        """åˆå§‹åŒ–"""
        # ç¢ºä¿æ‰€æœ‰ç›®éŒ„å­˜åœ¨
        ensure_directories()

        logger.info("Layer 1 æµæ°´ç·šåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  â€¢ åŸºç¤ç›®éŒ„: {LAYER1_BASE_DIR}")
        logger.info(f"  â€¢ Excel A: {EXCEL_A_PATH}")
        logger.info(f"  â€¢ Excel B: {EXCEL_B_PATH}")
        logger.info(f"  â€¢ Excel C: {EXCEL_C_PATH}")

    def run_full_pipeline(self) -> Dict:
        """
        é‹è¡Œå®Œæ•´ Layer 1 æµæ°´ç·š

        æµç¨‹:
        1. æª¢æŸ¥ Excel A æ˜¯å¦æœ‰æ¨™è¨»
        2. æ‰¹é‡ä¸‹è¼‰è¦–é »ä¸¦è‡ªå‹•åˆ†é¡
        3. ç‰¹å¾µæå– â†’ Excel B
        4. å¤§æ•¸æ“šåˆ†æ â†’ Excel C
        5. æ¨¡çµ„å„ªåŒ–

        Returns:
            åŸ·è¡Œçµ±è¨ˆ
        """
        logger.info(f"\n{'='*80}")
        logger.info("ğŸš€ TSAR-RAPTOR Layer 1 äººå·¥ä¸»å°æ¨™è¨»æµæ°´ç·š - å•Ÿå‹•")
        logger.info(f"{'='*80}\n")

        stats = {}

        # Step 1: æª¢æŸ¥ Excel A
        if not EXCEL_A_PATH.exists():
            logger.error(f"âŒ Excel A ä¸å­˜åœ¨: {EXCEL_A_PATH}")
            logger.error("   è«‹å…ˆä½¿ç”¨ Chrome æ“´å±•é€²è¡Œæ¨™è¨»")
            return {}

        # Step 2: æ‰¹é‡ä¸‹è¼‰ä¸¦è‡ªå‹•åˆ†é¡
        logger.info("ğŸ“¥ [Step 1/4] æ‰¹é‡ä¸‹è¼‰ä¸¦è‡ªå‹•åˆ†é¡è¦–é »...")
        downloader = TikTokDownloaderClassified(
            excel_a_path=str(EXCEL_A_PATH),
            max_workers=4
        )
        download_stats = downloader.download_from_excel_a()
        stats['download'] = download_stats

        # Step 3: ç‰¹å¾µæå–
        logger.info("\nğŸ”¬ [Step 2/4] ç‰¹å¾µæå–...")
        extractor = FeatureExtractorLayer1(
            output_excel_b=str(EXCEL_B_PATH),
            max_workers=4,
            sample_frames=30
        )
        df_features = extractor.batch_extract()
        stats['features'] = {'total': len(df_features)}

        # Step 4: å¤§æ•¸æ“šåˆ†æ
        logger.info("\nğŸ“Š [Step 3/4] å¤§æ•¸æ“šåˆ†æ...")
        analyzer = BigDataAnalyzer(
            excel_b_path=str(EXCEL_B_PATH),
            output_excel_c=str(EXCEL_C_PATH)
        )
        analysis_results = analyzer.analyze()
        stats['analysis'] = {'features_analyzed': len(analysis_results.get('ranked_features', []))}

        # Step 5: æ¨¡çµ„å„ªåŒ–
        logger.info("\nâš™ï¸  [Step 4/4] æ¨¡çµ„è‡ªå‹•å„ªåŒ–...")
        optimized_config_path = LAYER1_DATA_DIR / "optimized_config.json"
        optimizer = ModuleOptimizer(
            excel_c_path=str(EXCEL_C_PATH),
            config_output=str(optimized_config_path)
        )
        optimized_config = optimizer.optimize()
        stats['optimization'] = {'modules_optimized': len(optimized_config.get('module_weights', {}))}

        # æœ€çµ‚çµ±è¨ˆ
        logger.info(f"\n{'='*80}")
        logger.info("âœ… Layer 1 æµæ°´ç·šå®Œæˆï¼")
        logger.info(f"{'='*80}")
        logger.info(f"  â€¢ ä¸‹è¼‰è¦–é »: {download_stats.get('success', 0)} æˆåŠŸ, {download_stats.get('failed', 0)} å¤±æ•—")
        if 'by_category' in download_stats:
            logger.info(f"    åˆ†é¡çµ±è¨ˆ:")
            logger.info(f"      - Real: {download_stats['by_category']['real']}")
            logger.info(f"      - AI: {download_stats['by_category']['ai']}")
            logger.info(f"      - Uncertain: {download_stats['by_category']['uncertain']}")
            logger.info(f"      - Movies: {download_stats['by_category']['exclude']}")
        logger.info(f"  â€¢ æå–ç‰¹å¾µ: {len(df_features)} å€‹è¦–é »")
        logger.info(f"  â€¢ åˆ†æç‰¹å¾µ: {len(analysis_results.get('ranked_features', []))} å€‹ç‰¹å¾µ")
        logger.info(f"  â€¢ å„ªåŒ–æ¨¡çµ„: {len(optimized_config.get('module_weights', {}))} å€‹æ¨¡çµ„")
        logger.info(f"{'='*80}\n")

        return stats


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(description="Layer 1 äººå·¥ä¸»å°æ¨™è¨»æµæ°´ç·š")

    parser.add_argument(
        '--check-paths',
        action='store_true',
        help='æª¢æŸ¥è·¯å¾‘é…ç½®'
    )

    args = parser.parse_args()

    # å‰µå»ºæµæ°´ç·š
    pipeline = Layer1Pipeline()

    if args.check_paths:
        print(f"\n{'='*80}")
        print("è·¯å¾‘é…ç½®:")
        print(f"{'='*80}")
        print(f"åŸºç¤ç›®éŒ„: {LAYER1_BASE_DIR}")
        print(f"æ•¸æ“šç›®éŒ„: {LAYER1_DATA_DIR}")
        print(f"\nExcel æ–‡ä»¶:")
        print(f"  â€¢ Excel A: {EXCEL_A_PATH}")
        print(f"  â€¢ Excel B: {EXCEL_B_PATH}")
        print(f"  â€¢ Excel C: {EXCEL_C_PATH}")
        print(f"\nè¦–é »æ–‡ä»¶å¤¾:")
        from config import LAYER1_VIDEO_FOLDERS
        for label, folder in LAYER1_VIDEO_FOLDERS.items():
            print(f"  â€¢ {label}: {folder}")
        print(f"{'='*80}\n")
        return

    # åŸ·è¡Œå®Œæ•´æµæ°´ç·š
    stats = pipeline.run_full_pipeline()

    if stats:
        print(f"\nâœ… Layer 1 æµæ°´ç·šåŸ·è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
