#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TSAR-RAPTOR Layer 1 Pipeline
äººå·¥ä¸»å°æ¨™è¨»å®Œæ•´æµæ°´ç·š

è¨­è¨ˆåŸå‰‡:
- ç¬¬ä¸€æ€§åŸç†: äººé¡åˆ¤å®š â†’ æ•¸æ“šåˆ†æ â†’ æ¨¡çµ„å„ªåŒ–
- æ²™çš‡ç‚¸å½ˆ: ç´šè¯å­¸ç¿’ï¼Œæ•¸æ“šé©…å‹•
- çŒ›ç¦½3: ä¸€éµåŸ·è¡Œï¼Œå…¨è‡ªå‹•

å®Œæ•´æµç¨‹:
1. Chromeæ“´å±•æ¨™è¨» â†’ Excel A
2. æ‰¹é‡ä¸‹è¼‰ä¸¦è‡ªå‹•åˆ†é¡åˆ°æ–‡ä»¶å¤¾
3. ç‰¹å¾µæå– â†’ Excel B
4. å¤§æ•¸æ“šåˆ†æ â†’ Excel C
5. æ¨¡çµ„è‡ªå‹•å„ªåŒ–
"""

import sys
from pathlib import Path
import logging
import argparse
from typing import Dict

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å°å…¥é…ç½®
from config import (
    EXCEL_A_PATH, EXCEL_B_PATH, EXCEL_C_PATH,
    LAYER1_BASE_DIR, LAYER1_DATA_DIR,
    ensure_directories
)

# å°å…¥å„çµ„ä»¶
from downloader.tiktok_downloader_classified import TikTokDownloaderClassified

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Layer1Pipeline:
    """Layer 1 è‡ªæˆ‘å­¸ç¿’æµæ°´ç·šç¸½æ§"""

    def __init__(self):
        """åˆå§‹åŒ–"""
        # ç¢ºä¿æ‰€æœ‰ç›®éŒ„å­˜åœ¨
        ensure_directories()

        logger.info("Layer 1 æµæ°´ç·šåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  â€¢ åŸºç¤ç›®éŒ„: {LAYER1_BASE_DIR}")
        logger.info(f"  â€¢ Excel A: {EXCEL_A_PATH}")
        logger.info(f"  â€¢ Excel B: {EXCEL_B_PATH}")
        logger.info(f"  â€¢ Excel C: {EXCEL_C_PATH}")

    def run_redo_download(self, download: bool = True) -> Dict:
        from datetime import datetime
        import shutil
        from config import LAYER1_VIDEO_FOLDERS

        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_root = (LAYER1_DATA_DIR / 'redo_backup' / ts)
        backup_root.mkdir(parents=True, exist_ok=True)

        moved = 0
        moved_by = {k: 0 for k in LAYER1_VIDEO_FOLDERS.keys()}

        for folder_key, folder in LAYER1_VIDEO_FOLDERS.items():
            if not folder.exists():
                continue
            target_dir = backup_root / folder_key
            target_dir.mkdir(parents=True, exist_ok=True)
            for p in folder.glob('*.mp4'):
                try:
                    shutil.move(str(p), str(target_dir / p.name))
                    moved += 1
                    moved_by[folder_key] = moved_by.get(folder_key, 0) + 1
                except Exception as e:
                    logger.warning(f"âš ï¸  å‚™ä»½å¤±æ•—: {p} ({e})")

        moved_data = 0
        data_target = backup_root / "_data"
        for p in LAYER1_DATA_DIR.rglob('*.mp4'):
            try:
                rp = p.resolve()
                if rp.is_relative_to(backup_root.resolve()):
                    continue
            except Exception:
                pass

            try:
                rel = p.relative_to(LAYER1_DATA_DIR)
                dst = data_target / rel
                dst.parent.mkdir(parents=True, exist_ok=True)
                shutil.move(str(p), str(dst))
                moved_data += 1
            except Exception as e:
                logger.warning(f"âš ï¸  å‚™ä»½å¤±æ•—: {p} ({e})")

        if not download:
            logger.info("ğŸ“¦ å·²å®Œæˆå‚™ä»½ï¼ˆè·³éé‡æ–°ä¸‹è¼‰ï¼‰")
            return {
                'backup_dir': str(backup_root),
                'moved': moved,
                'moved_by': moved_by,
                'moved_data': moved_data,
                'download': {'skipped': True}
            }

        logger.info("ğŸ“¥ é‡åšä¸‹è¼‰ï¼šå·²å‚™ä»½ç¾æœ‰å½±ç‰‡ï¼Œé–‹å§‹é‡æ–°ä¸‹è¼‰...")
        downloader = TikTokDownloaderClassified(
            excel_a_path=str(EXCEL_A_PATH),
            max_workers=8
        )
        download_stats = downloader.download_from_excel_a()
        return {
            'backup_dir': str(backup_root),
            'moved': moved,
            'moved_by': moved_by,
            'moved_data': moved_data,
            'download': download_stats
        }

    def run_full_pipeline(self) -> Dict:
        """
        é‹è¡Œå®Œæ•´ Layer 1 æµæ°´ç·š

        æµç¨‹:
        1. æª¢æŸ¥ Excel A æ˜¯å¦æœ‰æ¨™è¨»
        2. æ‰¹é‡ä¸‹è¼‰è¦–é »ä¸¦è‡ªå‹•åˆ†é¡
        3. ç‰¹å¾µæå– â†’ Excel B
        4. å¤§æ•¸æ“šåˆ†æ â†’ Excel C
        5. æ¨¡çµ„å„ªåŒ–

        Returns:
            åŸ·è¡Œçµ±è¨ˆ
        """
        logger.info(f"\n{'='*80}")
        logger.info("ğŸš€ TSAR-RAPTOR Layer 1 äººå·¥ä¸»å°æ¨™è¨»æµæ°´ç·š - å•Ÿå‹•")
        logger.info(f"{'='*80}\n")

        stats = {}

        # Step 1: æª¢æŸ¥ Excel A
        if not EXCEL_A_PATH.exists():
            logger.error(f"âŒ Excel A ä¸å­˜åœ¨: {EXCEL_A_PATH}")
            logger.error("   è«‹å…ˆä½¿ç”¨ Chrome æ“´å±•é€²è¡Œæ¨™è¨»")
            return {}

        # Step 2: æ‰¹é‡ä¸‹è¼‰ä¸¦è‡ªå‹•åˆ†é¡
        logger.info("ğŸ“¥ [Step 1/4] æ‰¹é‡ä¸‹è¼‰ä¸¦è‡ªå‹•åˆ†é¡è¦–é »...")
        downloader = TikTokDownloaderClassified(
            excel_a_path=str(EXCEL_A_PATH),
            max_workers=8
        )
        download_stats = downloader.download_from_excel_a()
        stats['download'] = download_stats

        from analyzer.feature_extractor_layer1 import FeatureExtractorLayer1
        from analyzer.big_data_analyzer import BigDataAnalyzer
        from auto_reconstructor.module_optimizer import ModuleOptimizer

        # Step 3: ç‰¹å¾µæå–
        logger.info("\nğŸ”¬ [Step 2/4] ç‰¹å¾µæå–...")
        extractor = FeatureExtractorLayer1(
            output_excel_b=str(EXCEL_B_PATH),
            max_workers=4,
            sample_frames=30
        )
        df_features = extractor.batch_extract()
        stats['features'] = {'total': len(df_features)}

        # Step 4: å¤§æ•¸æ“šåˆ†æ
        logger.info("\nğŸ“Š [Step 3/4] å¤§æ•¸æ“šåˆ†æ...")
        analyzer = BigDataAnalyzer(
            excel_b_path=str(EXCEL_B_PATH),
            output_excel_c=str(EXCEL_C_PATH)
        )
        analysis_results = analyzer.analyze()
        stats['analysis'] = {'features_analyzed': len(analysis_results.get('ranked_features', []))}

        # Step 5: æ¨¡çµ„å„ªåŒ–
        logger.info("\nâš™ï¸  [Step 4/4] æ¨¡çµ„è‡ªå‹•å„ªåŒ–...")
        optimized_config_path = LAYER1_DATA_DIR / "optimized_config.json"
        optimizer = ModuleOptimizer(
            excel_c_path=str(EXCEL_C_PATH),
            config_output=str(optimized_config_path)
        )
        optimized_config = optimizer.optimize()
        stats['optimization'] = {'modules_optimized': len(optimized_config.get('module_weights', {}))}

        # æœ€çµ‚çµ±è¨ˆ
        logger.info(f"\n{'='*80}")
        logger.info("âœ… Layer 1 æµæ°´ç·šå®Œæˆï¼")
        logger.info(f"{'='*80}")
        logger.info(f"  â€¢ ä¸‹è¼‰è¦–é »: {download_stats.get('success', 0)} æˆåŠŸ, {download_stats.get('failed', 0)} å¤±æ•—")
        if 'by_category' in download_stats:
            logger.info(f"    åˆ†é¡çµ±è¨ˆ:")
            logger.info(f"      - Real: {download_stats['by_category']['real']}")
            logger.info(f"      - AI: {download_stats['by_category']['ai']}")
            logger.info(f"      - Uncertain: {download_stats['by_category']['uncertain']}")
            logger.info(f"      - Movies: {download_stats['by_category']['exclude']}")
        logger.info(f"  â€¢ æå–ç‰¹å¾µ: {len(df_features)} å€‹è¦–é »")
        logger.info(f"  â€¢ åˆ†æç‰¹å¾µ: {len(analysis_results.get('ranked_features', []))} å€‹ç‰¹å¾µ")
        logger.info(f"  â€¢ å„ªåŒ–æ¨¡çµ„: {len(optimized_config.get('module_weights', {}))} å€‹æ¨¡çµ„")
        logger.info(f"{'='*80}\n")

        return stats

    def run_download_only(self) -> Dict:
        if not EXCEL_A_PATH.exists():
            logger.error(f"âŒ Excel A ä¸å­˜åœ¨: {EXCEL_A_PATH}")
            logger.error("   è«‹å…ˆä½¿ç”¨ Chrome æ“´å±•é€²è¡Œæ¨™è¨»")
            return {}

        logger.info("ğŸ“¥ æ‰¹é‡ä¸‹è¼‰ä¸¦è‡ªå‹•åˆ†é¡è¦–é »ï¼ˆdownload-onlyï¼‰...")
        downloader = TikTokDownloaderClassified(
            excel_a_path=str(EXCEL_A_PATH),
            max_workers=8
        )
        download_stats = downloader.download_from_excel_a()
        logger.info(f"  â€¢ ä¸‹è¼‰è¦–é »: {download_stats.get('success', 0)} æˆåŠŸ, {download_stats.get('failed', 0)} å¤±æ•—")
        return {'download': download_stats}

    def run_download_detect_report(self) -> Dict:
        import json
        import os
        import re
        import shutil
        import subprocess
        import hashlib
        from datetime import datetime
        import pandas as pd

        if not EXCEL_A_PATH.exists():
            logger.error(f"âŒ Excel A ä¸å­˜åœ¨: {EXCEL_A_PATH}")
            return {}

        downloader = TikTokDownloaderClassified(
            excel_a_path=str(EXCEL_A_PATH),
            max_workers=8
        )
        download_stats = downloader.download_from_excel_a()

        df_a = pd.read_excel(EXCEL_A_PATH)
        url_col = 'å½±ç‰‡ç¶²å€' if 'å½±ç‰‡ç¶²å€' in df_a.columns else 'url'
        label_col = 'åˆ¤å®šçµæœ' if 'åˆ¤å®šçµæœ' in df_a.columns else 'label'
        video_id_col = 'è¦–é »ID' if 'è¦–é »ID' in df_a.columns else 'video_id'

        def _vid_from_url(u: str) -> str:
            m = re.search(r'/video/(\d+)', str(u))
            if m:
                return m.group(1)
            url_str = str(u or '').strip()
            if url_str:
                return hashlib.md5(url_str.encode('utf-8')).hexdigest()[:10]
            return ''

        def _canon(v: str) -> str:
            s = str(v).strip()
            s = re.sub(r'\.0$', '', s)
            if s.isdigit():
                try:
                    return str(int(s))
                except Exception:
                    return s.lstrip('0') or '0'
            if re.fullmatch(r'[0-9a-fA-F]{10}', s):
                return s.lower()
            return ''

        df_a[url_col] = df_a[url_col].astype(str)
        df_a[label_col] = df_a[label_col].astype(str)
        if video_id_col in df_a.columns:
            df_a[video_id_col] = df_a[video_id_col].astype(str)
        else:
            df_a[video_id_col] = ''

        df_a['__vid'] = df_a.apply(lambda r: _canon(_vid_from_url(r.get(url_col, '')) or str(r.get(video_id_col, '')).strip()), axis=1)

        label_map = dict(zip(df_a['__vid'].astype(str), df_a[label_col].astype(str).str.upper()))
        url_map = dict(zip(df_a['__vid'].astype(str), df_a[url_col].astype(str)))

        run_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        staging_root = (LAYER1_DATA_DIR / 'layer1_detection_runs' / run_id)
        staging_input = (staging_root / 'input')
        staging_output = (staging_root / 'output')
        staging_data = (staging_output / 'data')
        staging_input.mkdir(parents=True, exist_ok=True)
        staging_output.mkdir(parents=True, exist_ok=True)
        staging_data.mkdir(parents=True, exist_ok=True)

        videos = []
        for folder_key in ['real', 'ai', 'uncertain']:
            folder = (LAYER1_BASE_DIR / folder_key) if folder_key != 'uncertain' else (LAYER1_BASE_DIR / 'not sure')
            if not folder.exists():
                continue
            for video_path in folder.glob('*.mp4'):
                video_id = _canon(video_path.stem.split('_')[-1])
                if not video_id:
                    continue
                dst = staging_input / f'{video_id}.mp4'
                if not dst.exists():
                    shutil.copy2(video_path, dst)
                videos.append(video_id)

        videos = sorted(set(videos))

        if not videos:
            logger.warning('âš ï¸  æ²’æœ‰å¯ç”¨çš„è¦–é »åšæª¢æ¸¬')
            return {'download': download_stats, 'evaluated': 0}

        repo_root = project_root.parent
        env = os.environ.copy()
        env['INPUT_DIR'] = str(staging_input)
        env['OUTPUT_DIR'] = str(staging_output)
        env['DATA_DIR'] = str(staging_data)
        env['MAX_TIME'] = str(env.get('MAX_TIME', '600'))

        result = subprocess.run(
            [sys.executable, str(repo_root / 'autotesting.py')],
            cwd=str(repo_root),
            env=env,
            capture_output=True,
            text=True,
            timeout=60 * 60
        )

        if result.returncode != 0:
            logger.error('âŒ AIæª¢æ¸¬åŸ·è¡Œå¤±æ•—')
            logger.error(result.stderr[-2000:])
            return {'download': download_stats, 'evaluated': 0, 'error': 'detection_failed'}

        rows = []
        for diag_path in staging_output.glob('diagnostic_*.json'):
            try:
                data = json.loads(diag_path.read_text(encoding='utf-8'))
            except Exception:
                continue

            fp = str(data.get('file_path', ''))
            vid = Path(fp).stem
            ai_p = float(data.get('global_probability', 0.0))
            bitrate = int(data.get('video_characteristics', {}).get('bitrate', 0))
            face = float(data.get('video_characteristics', {}).get('face_presence', 0.0))
            static_ratio = float(data.get('video_characteristics', {}).get('static_ratio', 0.0))

            if ai_p <= 30:
                pred = 'REAL'
            elif ai_p <= 75:
                pred = 'UNCERTAIN'
            else:
                pred = 'AI'

            human = label_map.get(str(vid), '')
            url = url_map.get(str(vid), '')

            module_scores = data.get('module_scores', {})
            top_modules = sorted(
                [(k, float(v)) for k, v in module_scores.items()],
                key=lambda x: x[1],
                reverse=True
            )[:3]

            rows.append({
                'video_id': str(vid),
                'url': url,
                'human_label': human,
                'ai_probability': ai_p,
                'pred_label': pred,
                'bitrate': bitrate,
                'face_presence': face,
                'static_ratio': static_ratio,
                'top_modules': json.dumps(top_modules, ensure_ascii=False)
            })

        if not rows:
            logger.error('âŒ AIæª¢æ¸¬å®Œæˆä½†æœªç”Ÿæˆä»»ä½• diagnostic_*.json')
            if result.stdout:
                logger.error(result.stdout[-2000:])
            if result.stderr:
                logger.error(result.stderr[-2000:])
            return {'download': download_stats, 'evaluated': 0, 'error': 'no_diagnostics'}

        df_eval = pd.DataFrame(rows)
        eval_xlsx = LAYER1_DATA_DIR / f'layer1_ai_eval_{run_id}.xlsx'
        df_eval.to_excel(eval_xlsx, index=False)

        def bucket(b: int) -> str:
            if b <= 0:
                return 'unknown'
            if b < 800_000:
                return '<0.8'
            if b < 1_500_000:
                return '0.8-1.5'
            if b < 2_000_000:
                return '1.5-2.0'
            return '>2.0'

        df_eval['bitrate_bucket'] = df_eval['bitrate'].apply(bucket)

        def normalize_label(x: str) -> str:
            x = str(x).strip().upper()
            if x in {'REAL', 'AI', 'UNCERTAIN', 'EXCLUDE'}:
                return x
            if x in {'NOT SURE', 'NOT_SURE'}:
                return 'UNCERTAIN'
            if x in {'MOVIE', 'MOVIE/ANIME', 'MOVIES'}:
                return 'EXCLUDE'
            return x

        df_eval['human_label'] = df_eval['human_label'].apply(normalize_label)

        considered = df_eval[df_eval['human_label'].isin(['REAL', 'AI'])].copy()
        tp = int(((considered['human_label'] == 'AI') & (considered['pred_label'] == 'AI')).sum())
        tn = int(((considered['human_label'] == 'REAL') & (considered['pred_label'] == 'REAL')).sum())
        fp = int(((considered['human_label'] == 'REAL') & (considered['pred_label'] == 'AI')).sum())
        fn = int(((considered['human_label'] == 'AI') & (considered['pred_label'] == 'REAL')).sum())

        accuracy = (tp + tn) / max(len(considered), 1)

        report_lines = []
        report_lines.append('=' * 80)
        report_lines.append('LAYER 1 AIæª¢æ¸¬å°æ¯”å ±å‘Š')
        report_lines.append('=' * 80)
        report_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"Excel A: {EXCEL_A_PATH}")
        report_lines.append(f"Eval Excel: {eval_xlsx}")
        report_lines.append('')
        report_lines.append(f"Evaluated videos: {len(df_eval)}")
        report_lines.append(f"Considered (REAL/AI): {len(considered)}")
        report_lines.append(f"Accuracy (REAL/AI only): {accuracy*100:.1f}%")
        report_lines.append('')
        report_lines.append('Confusion (REAL/AI only):')
        report_lines.append(f"  TP (AIâ†’AI): {tp}")
        report_lines.append(f"  TN (REALâ†’REAL): {tn}")
        report_lines.append(f"  FP (REALâ†’AI): {fp}")
        report_lines.append(f"  FN (AIâ†’REAL): {fn}")
        report_lines.append('')
        report_lines.append('Bitrate buckets (REAL/AI only):')

        for b, g in considered.groupby('bitrate_bucket'):
            g_tp = int(((g['human_label'] == 'AI') & (g['pred_label'] == 'AI')).sum())
            g_tn = int(((g['human_label'] == 'REAL') & (g['pred_label'] == 'REAL')).sum())
            g_fp = int(((g['human_label'] == 'REAL') & (g['pred_label'] == 'AI')).sum())
            g_fn = int(((g['human_label'] == 'AI') & (g['pred_label'] == 'REAL')).sum())
            report_lines.append(f"  {b}: n={len(g)}, FP={g_fp}, FN={g_fn}, TP={g_tp}, TN={g_tn}")

        report_lines.append('')
        report_text = '\n'.join(report_lines)

        report_dir = LAYER1_DATA_DIR / 'report'
        report_dir.mkdir(parents=True, exist_ok=True)
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')

        docx_path = report_dir / f'layer1_ai_eval_{ts}.docx'
        pdf_path = report_dir / f'layer1_ai_eval_{ts}.pdf'

        try:
            from docx import Document
            doc = Document()
            for line in report_text.split('\n'):
                doc.add_paragraph(line)
            doc.save(str(docx_path))
        except Exception as e:
            logger.warning(f"âš ï¸  Wordå ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            docx_path = None

        try:
            from reportlab.lib.pagesizes import A4
            from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
            from reportlab.lib.styles import getSampleStyleSheet

            styles = getSampleStyleSheet()
            story = []
            for line in report_text.split('\n'):
                story.append(Paragraph(line.replace('<', '&lt;').replace('>', '&gt;'), styles['Normal']))
                story.append(Spacer(1, 6))
            doc = SimpleDocTemplate(str(pdf_path), pagesize=A4)
            doc.build(story)
        except Exception as e:
            logger.warning(f"âš ï¸  PDFå ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            pdf_path = None

        return {
            'download': download_stats,
            'evaluated': len(df_eval),
            'eval_excel': str(eval_xlsx),
            'docx': str(docx_path) if docx_path else '',
            'pdf': str(pdf_path) if pdf_path else ''
        }


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(description="Layer 1 äººå·¥ä¸»å°æ¨™è¨»æµæ°´ç·š")

    parser.add_argument(
        '--check-paths',
        action='store_true',
        help='æª¢æŸ¥è·¯å¾‘é…ç½®'
    )

    parser.add_argument(
        '--download-detect-report',
        action='store_true',
        help='ä¸‹è¼‰ + AIæª¢æ¸¬ + å°æ¯”å ±å‘Šï¼ˆLayer 1ï¼‰'
    )

    parser.add_argument(
        '--download-only',
        action='store_true',
        help='åªä¸‹è¼‰ä¸¦åˆ†é¡ï¼ˆä¸åšæª¢æ¸¬/å ±å‘Šï¼‰'
    )

    parser.add_argument(
        '--redo-download',
        action='store_true',
        help='é‡åšä¸‹è¼‰ï¼ˆå‚™ä»½ç¾æœ‰å½±ç‰‡å¾Œé‡æ–°ä¸‹è¼‰ï¼‰'
    )

    parser.add_argument(
        '--redo-backup-only',
        action='store_true',
        help='åªåšé‡åšä¸‹è¼‰çš„å‚™ä»½ï¼ˆä¸ä¸‹è¼‰ï¼‰'
    )

    args = parser.parse_args()

    # å‰µå»ºæµæ°´ç·š
    pipeline = Layer1Pipeline()

    if args.check_paths:
        print(f"\n{'='*80}")
        print("è·¯å¾‘é…ç½®:")
        print(f"{'='*80}")
        print(f"åŸºç¤ç›®éŒ„: {LAYER1_BASE_DIR}")
        print(f"æ•¸æ“šç›®éŒ„: {LAYER1_DATA_DIR}")
        print(f"\nExcel æ–‡ä»¶:")
        print(f"  â€¢ Excel A: {EXCEL_A_PATH}")
        print(f"  â€¢ Excel B: {EXCEL_B_PATH}")
        print(f"  â€¢ Excel C: {EXCEL_C_PATH}")
        print(f"\nè¦–é »æ–‡ä»¶å¤¾:")
        from config import LAYER1_VIDEO_FOLDERS
        for label, folder in LAYER1_VIDEO_FOLDERS.items():
            print(f"  â€¢ {label}: {folder}")
        print(f"{'='*80}\n")
        return

    if args.download_detect_report:
        stats = pipeline.run_download_detect_report()
        if stats and not stats.get('error'):
            print("\nâœ… Layer 1 ä¸‹è¼‰+æª¢æ¸¬+å ±å‘Šå®Œæˆï¼")
        elif stats and stats.get('error'):
            print(f"\nâŒ Layer 1 ä¸‹è¼‰+æª¢æ¸¬+å ±å‘Šå¤±æ•—: {stats.get('error')}")
        return

    if args.download_only:
        stats = pipeline.run_download_only()
        if stats:
            print("\nâœ… Layer 1 ä¸‹è¼‰å®Œæˆï¼")
        return

    if args.redo_backup_only:
        stats = pipeline.run_redo_download(download=False)
        if stats:
            print("\nâœ… Layer 1 å‚™ä»½å®Œæˆï¼")
        return
    
    if args.redo_download:
        stats = pipeline.run_redo_download()
        if stats:
            print("\nâœ… Layer 1 é‡åšä¸‹è¼‰å®Œæˆï¼")
        return

    # åŸ·è¡Œå®Œæ•´æµæ°´ç·š
    stats = pipeline.run_full_pipeline()

    if stats:
        print(f"\nâœ… Layer 1 æµæ°´ç·šåŸ·è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
